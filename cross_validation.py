import numpy as np
from sklearn import preprocessing
import csv
import tensorflow as tf
from sklearn.model_selection import cross_val_predict, cross_val_score
from tensorflow import keras
from tensorflow.python.keras import optimizers, regularizers
from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

cale = "F:/Projects/IA/SmartphoneUserClassification/data/"

train_labels = []
ids = []
with open(cale + "train_labels.csv") as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count == 0:
            line_count += 1
        else:
            if line_count >= 10000:
                break
            ids.append(row[0])
            train_labels.append(row[1])
            line_count += 1
    print(f'Processed {line_count} lines.')

train_data = np.zeros((len(ids), 139, 3))

for i in range(len(ids)):
    with open(cale + "train/" + ids[i] + ".csv") as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        print(i)
        line = 0
        for row in csv_reader:
            if line == 139:
                break
            train_data[i, line, 0] = row[0]
            train_data[i, line, 1] = row[1]
            train_data[i, line, 2] = row[2]
            line += 1

train_data = train_data.astype(float)

train_labels = np.asarray(train_labels)
train_labels = train_labels.astype(float)
for i in range(train_labels.shape[0]):
    train_labels[i] -= 1

train_shape0 = train_data.shape[0]

scaler = preprocessing.StandardScaler()
scaler.fit(train_data.reshape([train_shape0, 139 * 3]))
train_scaled = scaler.transform(train_data.reshape([train_shape0, 139 * 3]))
train_data = train_scaled.reshape([train_shape0, 139, 3])


def create_network():
    # Weights initializer (to initialize weights from a normal distribution)
    initializer = keras.initializers.glorot_normal(seed=None)

    # Building the model
    model = keras.Sequential([
        # 2 conv layers with 100 filters and kernel dimension 10
        keras.layers.Conv1D(100, 10, activation='relu', input_shape=(139, 3)),
        keras.layers.Conv1D(100, 10, activation='relu'),
        # Max pooling, dividing the input map, generated by the conv layers, by 3
        keras.layers.MaxPooling1D(3),
        # Same concept applied again, but with global average pooling for dimensionality reduction
        keras.layers.Conv1D(160, 10, activation='relu'),
        keras.layers.Conv1D(160, 10, activation='relu'),
        keras.layers.GlobalAveragePooling1D(),
        # Dropout layer, giving each weight a 50% probability to become 0, avoiding over-fitting
        keras.layers.Dropout(0.5),
        # 2 hidden layers (with 128 and 64 nodes respectively) followed by the output layer
        keras.layers.Dense(128, activation=tf.nn.relu, kernel_initializer=initializer,
                           kernel_regularizer=regularizers.l1(0.01), activity_regularizer=regularizers.l2(0.01)),
        keras.layers.Dense(64, activation=tf.nn.relu, kernel_initializer=initializer,
                           kernel_regularizer=regularizers.l1(0.01), activity_regularizer=regularizers.l2(0.01)),
        keras.layers.Dense(20, activation=tf.nn.softmax)
    ])

    # Model compiling with adam optimizer (which gives an adaptive learning rate)
    opt = optimizers.Adam(lr=0.001)
    model.compile(optimizer=opt,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model


neural_network = KerasClassifier(build_fn=create_network, epochs=25)
# pred_labels = cross_val_predict(neural_network, train_data, train_labels, cv=3)
# conf_matrix = confusion_matrix(train_labels, pred_labels)
#
# plot_cells_matrix = []
# for i in range(conf_matrix.shape[0]):
#     line = []
#     for j in range(conf_matrix.shape[1]):
#         line.append(str(conf_matrix[i][j]))
#     plot_cells_matrix.append(line)
#
# plt.figure()
# tb = plt.table(cellText=plot_cells_matrix, loc=(0, 0), cellLoc='center')
# ax = plt.gca()
# ax.set_xticks([])
# ax.set_yticks([])
# plt.show()

scores = cross_val_score(neural_network, train_data, train_labels, cv=3)
print(np.mean(scores))
