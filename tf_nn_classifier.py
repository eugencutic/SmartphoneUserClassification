import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
import csv
import tensorflow as tf
from tensorflow import keras
from tensorflow.python.keras import optimizers, regularizers

# Libraries no longer used, as early stopping proved inefficient, and plotting was necessary only once
from tensorflow.python.keras.callbacks import EarlyStopping, LearningRateScheduler
from tensorflow.python.keras.utils import plot_model


# Data loading
cale = "F:/Projects/IA/SmartphoneUserClassification/data/"

train_labels = []
ids = []
with open(cale + "train_labels.csv") as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count == 0:
            line_count += 1
        else:
            if line_count >= 10000:
                break
            # train_labels[row[0]] = row[1]
            ids.append(row[0])
            train_labels.append(row[1])
            line_count += 1
    print(f'Processed {line_count} lines.')

# Although some files had more than 139 lines, the data was truncated
# to avoid filling with fake data
train_data = np.zeros((len(ids), 139, 3))

for i in range(len(ids)):
    with open(cale + "train/" + ids[i] + ".csv") as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        print(i)
        line = 0
        for row in csv_reader:
            if line == 139:
                break
            train_data[i, line, 0] = row[0]
            train_data[i, line, 1] = row[1]
            train_data[i, line, 2] = row[2]
            line += 1

train_data = train_data.astype(float)

# Data splitting(on a 1/3 ratio for validation)
split_train_data = train_data[0:6000]
split_test_data = train_data[6000:]

split_train_labels = train_labels[0:6000]
split_train_labels = np.asarray(split_train_labels)
split_test_labels = train_labels[6000:]
split_test_labels = np.asarray(split_test_labels)

split_train_labels = split_train_labels.astype(float)
split_test_labels = split_test_labels.astype(float)

# The keras model considers the number of labels only, and so counts the labels
# starting from zero. Therefore, the train labels must be 0-19, instead of 1-20,
# as they are originally
for i in range(split_train_labels.shape[0]):
    split_train_labels[i] -= 1
for i in range(split_test_labels.shape[0]):
    split_test_labels[i] -= 1

# Data standardization
scaler = preprocessing.StandardScaler()
scaler.fit(split_train_data.reshape([6000, 139 * 3]))
train_scaled = scaler.transform(split_train_data.reshape([6000, 139 * 3]))
test_shape = split_test_data.shape[0]
scaler.fit(split_test_data.reshape([test_shape, 139 * 3]))
test_scaled = scaler.transform(split_test_data.reshape([test_shape, 139 * 3]))
split_train_data = train_scaled.reshape([6000, 139, 3])
split_test_data = test_scaled.reshape([test_shape, 139, 3])

# Weights initializer (to initialize weights from a normal distribution)
initializer = keras.initializers.glorot_normal(seed=None)

# Building the model
model = keras.Sequential([
    # 2 conv layers with 100 filters and kernel dimension 10
    keras.layers.Conv1D(100, 10, activation='relu', input_shape=(139, 3)),
    keras.layers.Conv1D(100, 10, activation='relu'),
    # Max pooling, dividing the input map, generated by the conv layers, by 3
    keras.layers.MaxPooling1D(3),
    # Same concept applied again, but with global average pooling for dimensionality reduction
    keras.layers.Conv1D(160, 10, activation='relu'),
    keras.layers.Conv1D(160, 10, activation='relu'),
    keras.layers.GlobalAveragePooling1D(),
    # Dropout layer, giving each weight a 50% probability to become 0, avoiding over-fitting
    keras.layers.Dropout(0.5),
    # 2 hidden layers (with 128 and 64 nodes respectively) followed by the output layer
    keras.layers.Dense(128, activation=tf.nn.relu, kernel_initializer=initializer,
                       kernel_regularizer=regularizers.l1(0.01), activity_regularizer=regularizers.l2(0.01)),
    keras.layers.Dense(64, activation=tf.nn.relu, kernel_initializer=initializer,
                       kernel_regularizer=regularizers.l1(0.01), activity_regularizer=regularizers.l2(0.01)),
    keras.layers.Dense(20, activation=tf.nn.softmax)
])

# Model compiling with adam optimizer (which gives an adaptive learning rate)
opt = optimizers.Adam(lr=0.001)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Training an plotting the results
history = model.fit(split_train_data, split_train_labels, validation_split=0.3, epochs=50)
print(history.history.keys())
plt.plot([i for i in range(1, 51)], history.history['val_acc'])
plt.plot([i for i in range(1, 51)], history.history['val_loss'], color='tab:red')
plt.show()

test_loss, test_acc = model.evaluate(split_test_data, split_test_labels)
print('Test accuracy:', test_acc)


